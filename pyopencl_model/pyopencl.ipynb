{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyopencl as cl\n",
    "import deviceinfo\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "import torch.nn.functional\n",
    "import torchvision.transforms as transforms\n",
    "from time import time\n",
    "PYOPENCL_COMPILER_OUTPUT=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model parameters into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to transfer: 20.001 sec\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "#CONV 0\n",
    "fp = open('../model/0_conv_weights_padded.txt', \"rb\")\n",
    "conv0_weights = np.loadtxt(fp, dtype='float32')\n",
    "fp = open('../model/0_conv_biases.txt', \"rb\")\n",
    "\n",
    "bn0_biases = np.loadtxt(fp, dtype='float32') \n",
    "fp = open('../model/0_conv_normalize.txt', \"rb\")\n",
    "bn0_weights = np.loadtxt(fp, dtype='float32',delimiter=',',usecols=0)\n",
    "fp = open('../model/0_conv_normalize.txt', \"rb\")\n",
    "bn0_mean = np.loadtxt(fp, dtype='float32',delimiter=',',usecols=1)\n",
    "fp = open('../model/0_conv_normalize.txt', \"rb\")\n",
    "bn0_var = np.loadtxt(fp, dtype='float32',delimiter=',',usecols=2)\n",
    "\n",
    "#CONV 2\n",
    "fp = open('../model/2_conv_weights.txt', \"rb\")\n",
    "conv2_weights = np.loadtxt(fp, dtype='float32')\n",
    "fp = open('../model/2_conv_biases.txt', \"rb\")\n",
    "bn2_biases = np.loadtxt(fp, dtype='float32')\n",
    "fp = open('../model/2_conv_normalize.txt', \"rb\")\n",
    "bn2_weights = np.loadtxt(fp, dtype='float32',delimiter=',',usecols=0)\n",
    "fp = open('../model/2_conv_normalize.txt', \"rb\")\n",
    "bn2_mean = np.loadtxt(fp, dtype='float32',delimiter=',',usecols=1)\n",
    "fp = open('../model/2_conv_normalize.txt', \"rb\")\n",
    "bn2_var = np.loadtxt(fp, dtype='float32',delimiter=',',usecols=2)\n",
    "\n",
    "#CONV 4\n",
    "fp = open('../model/4_conv_weights.txt', \"rb\")\n",
    "conv4_weights = np.loadtxt(fp, dtype='float32')\n",
    "fp = open('../model/4_conv_biases.txt', \"rb\")\n",
    "bn4_biases = np.loadtxt(fp, dtype='float32')\n",
    "fp = open('../model/4_conv_normalize.txt', \"rb\")\n",
    "bn4_weights = np.loadtxt(fp, dtype='float32',delimiter=',',usecols=0)\n",
    "fp = open('../model/4_conv_normalize.txt', \"rb\")\n",
    "bn4_mean = np.loadtxt(fp, dtype='float32',delimiter=',',usecols=1)\n",
    "fp = open('../model/4_conv_normalize.txt', \"rb\")\n",
    "bn4_var = np.loadtxt(fp, dtype='float32',delimiter=',',usecols=2)\n",
    "\n",
    "#CONV 6\n",
    "fp = open('../model/6_conv_weights.txt', \"rb\")\n",
    "conv6_weights = np.loadtxt(fp, dtype='float32')\n",
    "fp = open('../model/6_conv_biases.txt', \"rb\")\n",
    "bn6_biases = np.loadtxt(fp, dtype='float32')\n",
    "fp = open('../model/6_conv_normalize.txt', \"rb\")\n",
    "bn6_weights = np.loadtxt(fp, dtype='float32',delimiter=',',usecols=0)\n",
    "fp = open('../model/6_conv_normalize.txt', \"rb\")\n",
    "bn6_mean = np.loadtxt(fp, dtype='float32',delimiter=',',usecols=1)\n",
    "fp = open('../model/6_conv_normalize.txt', \"rb\")\n",
    "bn6_var = np.loadtxt(fp, dtype='float32',delimiter=',',usecols=2)\n",
    "\n",
    "#CONV 8\n",
    "fp = open('../model/8_conv_weights.txt', \"rb\")\n",
    "conv8_weights = np.loadtxt(fp, dtype='float32')\n",
    "fp = open('../model/8_conv_biases.txt', \"rb\")\n",
    "bn8_biases = np.loadtxt(fp, dtype='float32')\n",
    "fp = open('../model/8_conv_normalize.txt', \"rb\")\n",
    "bn8_weights = np.loadtxt(fp, dtype='float32',delimiter=',',usecols=0)\n",
    "fp = open('../model/8_conv_normalize.txt', \"rb\")\n",
    "bn8_mean = np.loadtxt(fp, dtype='float32',delimiter=',',usecols=1)\n",
    "fp = open('../model/8_conv_normalize.txt', \"rb\")\n",
    "bn8_var = np.loadtxt(fp, dtype='float32',delimiter=',',usecols=2)\n",
    "\n",
    "#CONV 10\n",
    "fp = open('../model/10_conv_weights.txt', \"rb\")\n",
    "conv10_weights = np.loadtxt(fp, dtype='float32')\n",
    "fp = open('../model/10_conv_biases.txt', \"rb\")\n",
    "bn10_biases = np.loadtxt(fp, dtype='float32')\n",
    "fp = open('../model/10_conv_normalize.txt', \"rb\")\n",
    "bn10_weights = np.loadtxt(fp, dtype='float32',delimiter=',',usecols=0)\n",
    "fp = open('../model/10_conv_normalize.txt', \"rb\")\n",
    "bn10_mean = np.loadtxt(fp, dtype='float32',delimiter=',',usecols=1)\n",
    "\n",
    "fp = open('../model/10_conv_normalize.txt', \"rb\")\n",
    "bn10_var = np.loadtxt(fp, dtype='float32',delimiter=',',usecols=2)\n",
    "\n",
    "#CONV 12\n",
    "fp = open('../model/12_conv_weights.txt', \"rb\")\n",
    "conv12_weights = np.loadtxt(fp, dtype='float32')\n",
    "fp = open('../model/12_conv_biases.txt', \"rb\")\n",
    "bn12_biases = np.loadtxt(fp, dtype='float32')\n",
    "fp = open('../model/12_conv_normalize.txt', \"rb\")\n",
    "bn12_weights = np.loadtxt(fp, dtype='float32',delimiter=',',usecols=0)\n",
    "fp = open('../model/12_conv_normalize.txt', \"rb\")\n",
    "bn12_mean = np.loadtxt(fp, dtype='float32',delimiter=',',usecols=1)\n",
    "fp = open('../model/12_conv_normalize.txt', \"rb\")\n",
    "bn12_var = np.loadtxt(fp, dtype='float32',delimiter=',',usecols=2)\n",
    "\n",
    "#CONV 14\n",
    "fp = open('../model/14_conv_weights.txt', \"rb\")\n",
    "conv14_weights = np.loadtxt(fp, dtype='float32')\n",
    "fp = open('../model/14_conv_biases.txt', \"rb\")\n",
    "bn14_biases = np.loadtxt(fp, dtype='float32')\n",
    "bn14_mean = np.zeros(1000, dtype=np.float32)\n",
    "bn14_var  = np.ones(1000, dtype=np.float32)\n",
    "bn14_weights = np.ones(1000, dtype=np.float32)\n",
    "\n",
    "t = time() - t\n",
    "print(\"Time taken to transfer: {:.3f} sec\".format(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.188235, 0.2     , 0.211765, ..., 0.4     , 0.423529, 0.423529],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# im_path = r'../images/ILSVRC2012_val_00007050.JPEG'\n",
    "# im = Image.open(im_path)\n",
    "\n",
    "\n",
    "\n",
    "# im.resize((256,256))\n",
    "# imshow(im)\n",
    "# # #preprocessing of the imput image\n",
    "# # transform = transforms.Compose([\n",
    "# #     transforms.Resize((256,256)),\n",
    "# #    transforms.ToTensor()\n",
    "# # ])\n",
    "# # im_input = transform(im)\n",
    "# h_input = im_input.numpy().reshape(-1).astype(np.float32)\n",
    "# h_input\n",
    "fp = open('output.txt', 'r')\n",
    "h_input = np.loadtxt(fp,  dtype='float32')\n",
    "h_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empty numpy arrays to store outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h_conv0_out = np.empty(16*256*256).astype(np.float32)\n",
    "h_bn0_out = np.empty(16*256*256).astype(np.float32)\n",
    "h_pool1_out = np.empty(16*128*128).astype(np.float32)\n",
    "\n",
    "h_conv2_out = np.empty(32*128*128).astype(np.float32)\n",
    "h_bn2_out = np.empty(32*128*128).astype(np.float32)\n",
    "h_pool3_out = np.empty(32*64*64).astype(np.float32)\n",
    "\n",
    "h_conv4_out = np.empty(64*64*64).astype(np.float32)\n",
    "h_bn4_out = np.empty(64*64*64).astype(np.float32)\n",
    "h_pool5_out = np.empty(64*32*32).astype(np.float32)\n",
    "\n",
    "h_conv6_out = np.empty(128*32*32).astype(np.float32)\n",
    "h_bn6_out = np.empty(128*32*32).astype(np.float32)\n",
    "h_pool7_out = np.empty(128*16*16).astype(np.float32)\n",
    "\n",
    "h_conv8_out = np.empty(256*16*16).astype(np.float32)\n",
    "h_bn8_out = np.empty(256*16*16).astype(np.float32)\n",
    "h_pool9_out = np.empty(256*8*8).astype(np.float32)\n",
    "\n",
    "h_conv10_out = np.empty(512*8*8).astype(np.float32)\n",
    "h_bn10_out = np.empty(512*8*8).astype(np.float32)\n",
    "h_pool11_out = np.empty(512*4*4).astype(np.float32)\n",
    "\n",
    "h_conv12_out = np.empty(1024*4*4).astype(np.float32)\n",
    "h_bn12_out = np.empty(1024*4*4).astype(np.float32)\n",
    "h_pool13_out = np.empty(1024*1*1).astype(np.float32)\n",
    "\n",
    "h_conv14_out = np.empty(1*1000).astype(np.float32)\n",
    "h_bn14_out = np.empty(1*1000).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Context and Command Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Device is GeForce MX250GPU from NVIDIA Corporation with a max of 3 compute units\n"
     ]
    }
   ],
   "source": [
    "context = cl.create_some_context()\n",
    "deviceinfo.output_device_info(context.devices[0])\n",
    "queue = cl.CommandQueue(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Buffers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sample = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = h_input)\n",
    "\n",
    "d_conv0_weights = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = conv0_weights)\n",
    "d_conv0_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_conv0_out.nbytes)\n",
    "d_bn0_weights = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn0_weights)\n",
    "d_bn0_biases = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn0_biases)\n",
    "d_bn0_mean = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn0_mean)\n",
    "d_bn0_var = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn0_var)\n",
    "d_bn0_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_bn0_out.nbytes)\n",
    "d_pool1_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_pool1_out.nbytes)\n",
    "\n",
    "d_conv2_weights = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = conv2_weights)\n",
    "d_conv2_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_conv2_out.nbytes)\n",
    "d_bn2_weights = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn2_weights)\n",
    "d_bn2_biases = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn2_biases)\n",
    "d_bn2_mean = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn2_mean)\n",
    "d_bn2_var = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn2_var)\n",
    "d_bn2_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_bn2_out.nbytes)\n",
    "d_pool3_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_pool3_out.nbytes)\n",
    "\n",
    "d_conv4_weights = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = conv4_weights)\n",
    "d_conv4_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_conv4_out.nbytes)\n",
    "d_bn4_weights = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn4_weights)\n",
    "d_bn4_biases = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn4_biases)\n",
    "d_bn4_mean = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn4_mean)\n",
    "d_bn4_var = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn4_var)\n",
    "d_bn4_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_bn4_out.nbytes)\n",
    "d_pool5_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_pool5_out.nbytes)\n",
    "\n",
    "d_conv6_weights = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = conv6_weights)\n",
    "d_conv6_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_conv6_out.nbytes)\n",
    "d_bn6_weights = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn6_weights)\n",
    "d_bn6_biases = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn6_biases)\n",
    "d_bn6_mean = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn6_mean)\n",
    "d_bn6_var = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn6_var)\n",
    "d_bn6_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_bn6_out.nbytes)\n",
    "d_pool7_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_pool7_out.nbytes)\n",
    "\n",
    "d_conv8_weights = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = conv8_weights)\n",
    "d_conv8_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_conv8_out.nbytes)\n",
    "d_bn8_weights = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn8_weights)\n",
    "d_bn8_biases = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn8_biases)\n",
    "d_bn8_mean = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn8_mean)\n",
    "d_bn8_var = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn8_var)\n",
    "d_bn8_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_bn8_out.nbytes)\n",
    "d_pool9_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_pool9_out.nbytes)\n",
    "\n",
    "d_conv10_weights = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = conv10_weights)\n",
    "d_conv10_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_conv10_out.nbytes)\n",
    "d_bn10_weights = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn10_weights)\n",
    "d_bn10_biases = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn10_biases)\n",
    "d_bn10_mean = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn10_mean)\n",
    "d_bn10_var = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn10_var)\n",
    "d_bn10_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_bn10_out.nbytes)\n",
    "d_pool11_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_pool11_out.nbytes)\n",
    "\n",
    "d_conv12_weights = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = conv12_weights)\n",
    "d_conv12_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_conv12_out.nbytes)\n",
    "d_bn12_weights = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn12_weights)\n",
    "d_bn12_biases = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn12_biases)\n",
    "d_bn12_mean = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn12_mean)\n",
    "d_bn12_var = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn12_var)\n",
    "d_bn12_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_bn12_out.nbytes)\n",
    "d_pool13_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_pool13_out.nbytes)\n",
    "\n",
    "d_conv14_weights = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = conv14_weights)\n",
    "d_conv14_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_conv14_out.nbytes)\n",
    "d_bn14_weights = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn14_weights)\n",
    "d_bn14_biases = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn14_biases)\n",
    "d_bn14_mean = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn14_mean)\n",
    "d_bn14_var = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf = bn14_var)\n",
    "d_bn14_out = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_bn14_out.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the kernel program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernelsource = open(\"darknet.cl\").read()\n",
    "program = cl.Program(context, kernelsource).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the datatypes of the kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_cl = program.conv\n",
    "conv_cl.set_scalar_arg_dtypes([None, None, None, np.int32, np.int32, np.int32, np.int32, np.int32, np.int32, np.int32, np.int32])\n",
    "\n",
    "bn_cl = program.batchnorm\n",
    "bn_cl.set_scalar_arg_dtypes([None, None, None, None, None, None, np.int32 , np.float32, np.int32])\n",
    "\n",
    "maxpool_cl = program.maxpool\n",
    "maxpool_cl.set_scalar_arg_dtypes([None, None, np.int32, np.int32, np.int32, np.int32, np.int32])\n",
    "\n",
    "conv1x1_cl = program.conv1x1\n",
    "conv1x1_cl.set_scalar_arg_dtypes([None, None, None,np.int32, np.int32, np.int32, np.int32, np.int32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Darknet Reference Model\n",
    "\n",
    "<img src=\"../model_architecture.jpg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Execution Time: 63.969 ms\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "conv_cl(queue,(256*256,16), (8,8), d_sample, d_conv0_weights, d_conv0_out, 32, 3, 1, 1, 3, 256, 256*256, 256)\n",
    "bn_cl(queue, (16,256*256), None, d_conv0_out, d_bn0_weights, d_bn0_biases, d_bn0_mean, d_bn0_var, d_bn0_out, 256*256, 0.00001, 1)\n",
    "maxpool_cl(queue, (16,), None, d_bn0_out, d_pool1_out, 256, 2, 2, 128, 0)\n",
    "\n",
    "conv_cl(queue,(128*128,32), (8,8), d_pool1_out, d_conv2_weights, d_conv2_out, 16*3*3, 3, 1, 1, 16, 128, 128*128, 128)\n",
    "bn_cl(queue, (32,128*128), None, d_conv2_out, d_bn2_weights, d_bn2_biases, d_bn2_mean, d_bn2_var, d_bn2_out, 128*128, 0.00001, 1)\n",
    "maxpool_cl(queue, (32,), None, d_bn2_out, d_pool3_out, 128, 2, 2, 64, 0)\n",
    "\n",
    "conv_cl(queue,(64*64,64), (8,8), d_pool3_out, d_conv4_weights, d_conv4_out, 32*3*3, 3, 1, 1, 32, 64, 64*64, 64)\n",
    "bn_cl(queue, (64,64*64), None, d_conv4_out, d_bn4_weights, d_bn4_biases, d_bn4_mean, d_bn4_var, d_bn4_out, 64*64, 0.00001, 1)\n",
    "maxpool_cl(queue, (64,), None, d_bn4_out, d_pool5_out, 64, 2, 2, 32, 0)\n",
    "\n",
    "conv_cl(queue,(32*32,128), (8,8), d_pool5_out, d_conv6_weights, d_conv6_out, 64*3*3, 3, 1, 1, 64, 32, 32*32, 32)\n",
    "bn_cl(queue, (128,32*32), None, d_conv6_out, d_bn6_weights, d_bn6_biases, d_bn6_mean, d_bn6_var, d_bn6_out, 32*32, 0.00001, 1)\n",
    "maxpool_cl(queue, (128,), None, d_bn6_out, d_pool7_out, 32, 2, 2, 16, 0)\n",
    "\n",
    "conv_cl(queue,(16*16,256), (8,8), d_pool7_out, d_conv8_weights, d_conv8_out, 128*3*3, 3, 1, 1, 128, 16, 16*16, 16)\n",
    "bn_cl(queue, (256,16*16), None, d_conv8_out, d_bn8_weights, d_bn8_biases, d_bn8_mean, d_bn8_var, d_bn8_out, 16*16, 0.00001, 1)\n",
    "maxpool_cl(queue, (256,), None, d_bn8_out, d_pool9_out, 16, 2, 2, 8, 0)\n",
    "\n",
    "conv_cl(queue,(8*8,512), (8,8), d_pool9_out, d_conv10_weights, d_conv10_out, 256*3*3, 3, 1, 1, 256, 8, 8*8, 8)\n",
    "bn_cl(queue, (512,8*8), None, d_conv10_out, d_bn10_weights, d_bn10_biases, d_bn10_mean, d_bn10_var, d_bn10_out, 8*8, 0.00001, 1)\n",
    "maxpool_cl(queue, (512,), None, d_bn10_out, d_pool11_out, 8, 2, 2, 4, 0)\n",
    "\n",
    "conv_cl(queue,(4*4,1024), (8,8), d_pool11_out, d_conv12_weights, d_conv12_out, 512*3*3, 3, 1, 1, 512, 4, 4*4, 4)\n",
    "bn_cl(queue, (1024,4*4), None, d_conv12_out, d_bn12_weights, d_bn12_biases, d_bn12_mean, d_bn12_var, d_bn12_out, 4*4, 0.00001, 1)\n",
    "maxpool_cl(queue, (1024,), None, d_bn12_out, d_pool13_out, 4, 4, 1, 1, 1)\n",
    "\n",
    "conv1x1_cl(queue,(1000,1), None, d_pool13_out, d_conv14_weights, d_conv14_out, 1024, 1, 0, 1, 1)\n",
    "bn_cl(queue, (1000,1*1), None, d_conv14_out, d_bn14_weights, d_bn14_biases, d_bn14_mean, d_bn14_var, d_bn14_out, 1*1, 0.00000, 0)\n",
    "\n",
    "t = time() - t\n",
    "print(\"Model Execution Time: {:.3f} ms\".format(t*1000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.enqueue_copy(queue, h_bn14_out, d_bn14_out)\n",
    "output_tensor = torch.from_numpy(h_bn14_out)\n",
    "output = torch.nn.functional.softmax(output_tensor, dim=0)\n",
    "accuracy, index = torch.topk(output,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "\n",
      "CLASS : flatworm                   ACCURACY : 21.257%\n",
      "CLASS : eel                        ACCURACY : 18.896%\n",
      "CLASS : puffer                     ACCURACY : 14.740%\n",
      "CLASS : sea snake                  ACCURACY : 8.939%\n",
      "CLASS : spotted salamander         ACCURACY : 8.545%\n"
     ]
    }
   ],
   "source": [
    "fp = open('../imagenet_labels.list','r')\n",
    "lines = fp.readlines()\n",
    "arr_lines = [line.strip() for line in lines[:1000]]\n",
    "print(\"Results:\\n\")\n",
    "for acc, i in zip(accuracy, index):   \n",
    "    label = arr_lines[int(i)]\n",
    "    print(\"CLASS : {:25}  ACCURACY : {:.3f}%\".format(label,acc*100)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_bn14_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open('pool13.txt', 'w')\n",
    "np.savetxt(fp, h_pool13_out, '%f')\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
